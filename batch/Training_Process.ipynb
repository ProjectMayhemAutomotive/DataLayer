{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a01a1bd-ef07-4b2e-8877-55ef0dd5bf14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline for Taking in and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1258a8ac-8a31-4089-90ec-08bf719fd316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afe08e2-1f7d-4aad-887c-b2faac174ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS project_mayhem;\n",
    "USE CATALOG project_mayhem;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9eb790-c914-4c67-bd42-191eb5c7f95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS project_mayhem.source_data;\n",
    "CREATE SCHEMA IF NOT EXISTS project_mayhem.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS project_mayhem.silver;\n",
    "CREATE SCHEMA IF NOT EXISTS project_mayhem.gold;\n",
    "CREATE VOLUME IF NOT EXISTS project_mayhem.source_data.raw;\n",
    "CREATE VOLUME IF NOT EXISTS project_mayhem.source_data.models;\n",
    "CREATE VOLUME IF NOT EXISTS project_mayhem.gold.artifacts;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4fbb90-078e-4669-9e67-6ada1d354115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transforming Raw CSV Data towards Medallion Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a271dde-757d-458c-9346-dd04ace49748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"/Volumes/project_mayhem/source_data/raw/*.csv\"\n",
    "\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, inferSchema=True)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"mergreSchema\", \"true\").saveAsTable(\"project_mayhem.bronze.labeled_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eddca77-5cb0-4585-ac36-30c0905e022e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0e8ee4-9295-4203-8b9f-2680119806ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration loaded\n\nCatalog: project_mayhem\nBronze table: project_mayhem.bronze.labeled_data\nSilver table: project_mayhem.silver.processed_timeseries\nGold features table: project_mayhem.gold.ml_features\nGold stats table: project_mayhem.gold.normalization_stats\n\nFile storage volumes:\n  Raw data: /Volumes/project_mayhem/source_data/raw\n  Models: /Volumes/project_mayhem/source_data/models\n  Artifacts: /Volumes/project_mayhem/gold/artifacts\n\nDevice: cpu\nMLflow experiment: /Users/devforbchain@gmail.com/vehicle_health_training\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pyspark.sql import functions as F_spark\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Configuration for the training pipeline\n",
    "CATALOG = \"project_mayhem\"\n",
    "\n",
    "# Table references (three-part naming: catalog.schema.table)\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze.labeled_data\"\n",
    "SILVER_TABLE = f\"{CATALOG}.silver.processed_timeseries\"\n",
    "GOLD_FEATURES_TABLE = f\"{CATALOG}.gold.ml_features\"\n",
    "GOLD_STATS_TABLE = f\"{CATALOG}.gold.normalization_stats\"\n",
    "\n",
    "# Volume paths (these are file system paths within your volumes)\n",
    "# The format is /Volumes/catalog_name/schema_name/volume_name/\n",
    "RAW_DATA_VOLUME = f\"/Volumes/{CATALOG}/source_data/raw\"\n",
    "MODEL_VOLUME = f\"/Volumes/{CATALOG}/source_data/models\"\n",
    "ARTIFACTS_VOLUME = f\"/Volumes/{CATALOG}/gold/artifacts\"\n",
    "\n",
    "# For MLflow experiment tracking\n",
    "# You'll need to replace this with your actual Databricks user email\n",
    "CURRENT_USER = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "EXPERIMENT_NAME = f\"/Users/{CURRENT_USER}/vehicle_health_training\"\n",
    "\n",
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    \"window_size\": 60,\n",
    "    \"stride\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 12,\n",
    "    \"val_split\": 0.15,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"random_seed\": 42\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(CONFIG[\"random_seed\"])\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG[\"random_seed\"])\n",
    "\n",
    "print(\"configuration loaded\\n\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Bronze table: {BRONZE_TABLE}\")\n",
    "print(f\"Silver table: {SILVER_TABLE}\")\n",
    "print(f\"Gold features table: {GOLD_FEATURES_TABLE}\")\n",
    "print(f\"Gold stats table: {GOLD_STATS_TABLE}\")\n",
    "print(f\"\\nFile storage volumes:\")\n",
    "print(f\"  Raw data: {RAW_DATA_VOLUME}\")\n",
    "print(f\"  Models: {MODEL_VOLUME}\")\n",
    "print(f\"  Artifacts: {ARTIFACTS_VOLUME}\")\n",
    "print(f\"\\nDevice: {CONFIG['device']}\")\n",
    "print(f\"MLflow experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cb6fd5-49fd-45f9-a69b-1a2901f2a258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bronze to silver transformation...\nBronze table contains 288000 rows for 200 vehicles\nIdentified sensor columns: ['engine_temp', 'rpm', 'vibration', 'battery_v', 'speed']\nResampled data shape: (288000, 10)\nSilver layer created successfully: project_mayhem.silver.processed_timeseries\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>vehicle_id</th><th>engine_temp</th><th>rpm</th><th>vibration</th><th>battery_v</th><th>speed</th><th>engine_health</th><th>battery_health</th><th>urgency</th></tr></thead><tbody><tr><td>2025-01-01T00:00:00.000Z</td><td>veh_0</td><td>73.03946575199711</td><td>832.669045716939</td><td>0.11451599428450966</td><td>12.712044659960073</td><td>41.62090312619082</td><td>0.7203102488976955</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:01:00.000Z</td><td>veh_0</td><td>73.58894220018173</td><td>824.4464927458154</td><td>0.12291150295585745</td><td>12.711944659960073</td><td>43.82002717022266</td><td>0.6943612907519229</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:02:00.000Z</td><td>veh_0</td><td>73.42552556005242</td><td>823.1385374348702</td><td>0.11103380228909754</td><td>12.711844659960073</td><td>31.6788205032476</td><td>0.7208403027542646</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:03:00.000Z</td><td>veh_0</td><td>73.8549139896555</td><td>828.6522224069562</td><td>0.11215310947805036</td><td>12.711744659960074</td><td>46.147053822712664</td><td>0.7114452145496409</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:04:00.000Z</td><td>veh_0</td><td>72.80092185463594</td><td>820.4654455913756</td><td>0.11770292118185645</td><td>12.711644659960072</td><td>43.93612759382521</td><td>0.717912126725688</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:05:00.000Z</td><td>veh_0</td><td>74.26278407688547</td><td>821.5573346753303</td><td>0.12335638487813652</td><td>12.711544659960072</td><td>36.674432707859495</td><td>0.6822408289623025</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:06:00.000Z</td><td>veh_0</td><td>72.53770645782336</td><td>816.5287389250996</td><td>0.12113824937315458</td><td>12.711444659960073</td><td>43.028661992932996</td><td>0.7154283936233015</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:07:00.000Z</td><td>veh_0</td><td>74.12929461632753</td><td>816.1345922442816</td><td>0.11655173217636143</td><td>12.711344659960073</td><td>36.19213105554856</td><td>0.698074958708485</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:08:00.000Z</td><td>veh_0</td><td>72.81809572334583</td><td>802.945158512111</td><td>0.1390825136344327</td><td>12.711244659960073</td><td>37.80883340549487</td><td>0.6748667106753707</td><td>1.0</td><td>0</td></tr><tr><td>2025-01-01T00:09:00.000Z</td><td>veh_0</td><td>73.30906754112974</td><td>807.4799068178619</td><td>0.1273496632404339</td><td>12.711144659960073</td><td>34.496096408075985</td><td>0.6901495478336366</td><td>1.0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-01T00:00:00.000Z",
         "veh_0",
         73.03946575199711,
         832.669045716939,
         0.11451599428450966,
         12.712044659960073,
         41.62090312619082,
         0.7203102488976955,
         1.0,
         0
        ],
        [
         "2025-01-01T00:01:00.000Z",
         "veh_0",
         73.58894220018173,
         824.4464927458154,
         0.12291150295585745,
         12.711944659960073,
         43.82002717022266,
         0.6943612907519229,
         1.0,
         0
        ],
        [
         "2025-01-01T00:02:00.000Z",
         "veh_0",
         73.42552556005242,
         823.1385374348702,
         0.11103380228909754,
         12.711844659960073,
         31.6788205032476,
         0.7208403027542646,
         1.0,
         0
        ],
        [
         "2025-01-01T00:03:00.000Z",
         "veh_0",
         73.8549139896555,
         828.6522224069562,
         0.11215310947805036,
         12.711744659960074,
         46.147053822712664,
         0.7114452145496409,
         1.0,
         0
        ],
        [
         "2025-01-01T00:04:00.000Z",
         "veh_0",
         72.80092185463594,
         820.4654455913756,
         0.11770292118185645,
         12.711644659960072,
         43.93612759382521,
         0.717912126725688,
         1.0,
         0
        ],
        [
         "2025-01-01T00:05:00.000Z",
         "veh_0",
         74.26278407688547,
         821.5573346753303,
         0.12335638487813652,
         12.711544659960072,
         36.674432707859495,
         0.6822408289623025,
         1.0,
         0
        ],
        [
         "2025-01-01T00:06:00.000Z",
         "veh_0",
         72.53770645782336,
         816.5287389250996,
         0.12113824937315458,
         12.711444659960073,
         43.028661992932996,
         0.7154283936233015,
         1.0,
         0
        ],
        [
         "2025-01-01T00:07:00.000Z",
         "veh_0",
         74.12929461632753,
         816.1345922442816,
         0.11655173217636143,
         12.711344659960073,
         36.19213105554856,
         0.698074958708485,
         1.0,
         0
        ],
        [
         "2025-01-01T00:08:00.000Z",
         "veh_0",
         72.81809572334583,
         802.945158512111,
         0.1390825136344327,
         12.711244659960073,
         37.80883340549487,
         0.6748667106753707,
         1.0,
         0
        ],
        [
         "2025-01-01T00:09:00.000Z",
         "veh_0",
         73.30906754112974,
         807.4799068178619,
         0.1273496632404339,
         12.711144659960073,
         34.496096408075985,
         0.6901495478336366,
         1.0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "vehicle_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engine_temp",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "rpm",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "vibration",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "battery_v",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "speed",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "engine_health",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "battery_health",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "urgency",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_silver_layer():\n",
    "    \"\"\"\n",
    "    Transform bronze layer raw data into clean, resampled time series.\n",
    "    This function reads from bronze, applies cleaning and resampling, \n",
    "    and writes to silver layer.\n",
    "    \"\"\"\n",
    "    print(\"Starting bronze to silver transformation...\")\n",
    "    \n",
    "    # Read the bronze table\n",
    "    bronze_df = spark.table(BRONZE_TABLE)\n",
    "    \n",
    "    print(f\"Bronze table contains {bronze_df.count()} rows for {bronze_df.select('vehicle_id').distinct().count()} vehicles\")\n",
    "    \n",
    "    # First, let's validate and clean the data\n",
    "    # Remove any rows with null critical columns\n",
    "    cleaned_df = bronze_df.filter(\n",
    "        F_spark.col(\"vehicle_id\").isNotNull() & \n",
    "        F_spark.col(\"timestamp\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Cast timestamp to proper timestamp type if it isn't already\n",
    "    cleaned_df = cleaned_df.withColumn(\"timestamp\", F_spark.col(\"timestamp\").cast(\"timestamp\"))\n",
    "    \n",
    "    # Identify sensor columns (numeric columns excluding labels)\n",
    "    label_cols = [\"engine_health\", \"battery_health\", \"urgency\"]\n",
    "    all_cols = cleaned_df.columns\n",
    "    sensor_cols = [c for c in all_cols if c not in [\"vehicle_id\", \"timestamp\"] + label_cols]\n",
    "    \n",
    "    print(f\"Identified sensor columns: {sensor_cols}\")\n",
    "    \n",
    "    # For resampling in Spark, we'll use a different approach than pandas\n",
    "    # We'll create a complete time grid and join our data to it\n",
    "    \n",
    "    # First, get the time range for each vehicle\n",
    "    time_ranges = cleaned_df.groupBy(\"vehicle_id\").agg(\n",
    "        F_spark.min(\"timestamp\").alias(\"start_time\"),\n",
    "        F_spark.max(\"timestamp\").alias(\"end_time\")\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for easier time series handling\n",
    "    # For very large datasets, you'd want to process this in Spark using window functions\n",
    "    # But for typical vehicle telematics data, this approach works well\n",
    "    pdf = cleaned_df.toPandas()\n",
    "    \n",
    "    # Resample each vehicle's time series to 1-minute intervals\n",
    "    resampled_dfs = []\n",
    "    \n",
    "    for vehicle_id, group in pdf.groupby(\"vehicle_id\"):\n",
    "        # Sort by timestamp\n",
    "        group = group.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "        \n",
    "        # Create a regular time index (1 minute intervals)\n",
    "        start = group.index.min()\n",
    "        end = group.index.max()\n",
    "        regular_index = pd.date_range(start=start, end=end, freq=\"1min\")\n",
    "        \n",
    "        # Reindex to the regular intervals\n",
    "        group = group.reindex(regular_index)\n",
    "        \n",
    "        # Fill the vehicle_id column\n",
    "        group[\"vehicle_id\"] = vehicle_id\n",
    "        \n",
    "        # Interpolate sensor values linearly, then forward fill, then backward fill\n",
    "        # This handles gaps in the data gracefully\n",
    "        for col in sensor_cols + label_cols:\n",
    "            if col in group.columns:\n",
    "                group[col] = group[col].interpolate(method='time').ffill().bfill()\n",
    "        \n",
    "        resampled_dfs.append(group.reset_index().rename(columns={\"index\": \"timestamp\"}))\n",
    "    \n",
    "    # Combine all vehicles back together\n",
    "    resampled_pdf = pd.concat(resampled_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"Resampled data shape: {resampled_pdf.shape}\")\n",
    "    \n",
    "    # Convert back to Spark DataFrame\n",
    "    silver_df = spark.createDataFrame(resampled_pdf)\n",
    "    \n",
    "    # Write to silver layer as Delta table\n",
    "    # Using merge mode to handle incremental updates in the future\n",
    "    silver_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(SILVER_TABLE)\n",
    "    \n",
    "    print(f\"Silver layer created successfully: {SILVER_TABLE}\")\n",
    "    \n",
    "    return silver_df\n",
    "\n",
    "# Execute the transformation\n",
    "silver_df = create_silver_layer()\n",
    "display(silver_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47a4f14-07c9-42c3-8a49-1c66155bbc4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing normalization statistics...\nNormalization statistics:\n  engine_temp: mean=70.0553, std=2.6706\n  rpm: mean=794.9632, std=54.2376\n  vibration: mean=0.0992, std=0.0241\n  battery_v: mean=12.5231, std=0.0712\n  speed: mean=29.9115, std=6.0570\nNormalization statistics saved to project_mayhem.gold.normalization_stats\n"
     ]
    }
   ],
   "source": [
    "def compute_and_store_normalization_stats():\n",
    "    \"\"\"\n",
    "    Compute normalization statistics (mean and std) for each sensor column.\n",
    "    Store these in the gold layer for consistent use across training runs.\n",
    "    \"\"\"\n",
    "    print(\"Computing normalization statistics...\")\n",
    "    \n",
    "    # Read silver layer\n",
    "    silver_df = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    # Identify sensor columns\n",
    "    label_cols = [\"engine_health\", \"battery_health\", \"urgency\"]\n",
    "    sensor_cols = [c for c in silver_df.columns if c not in [\"vehicle_id\", \"timestamp\"] + label_cols]\n",
    "    \n",
    "    # Compute statistics for each sensor\n",
    "    stats_dict = {}\n",
    "    \n",
    "    for col in sensor_cols:\n",
    "        stats = silver_df.select(\n",
    "            F_spark.mean(col).alias(\"mean\"),\n",
    "            F_spark.stddev(col).alias(\"std\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        mean_val = float(stats[\"mean\"]) if stats[\"mean\"] is not None else 0.0\n",
    "        std_val = float(stats[\"std\"]) if stats[\"std\"] is not None else 1.0\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if std_val < 1e-9:\n",
    "            std_val = 1.0\n",
    "        \n",
    "        stats_dict[col] = {\"mean\": mean_val, \"std\": std_val}\n",
    "    \n",
    "    print(\"Normalization statistics:\")\n",
    "    for col, stats in stats_dict.items():\n",
    "        print(f\"  {col}: mean={stats['mean']:.4f}, std={stats['std']:.4f}\")\n",
    "    \n",
    "    # Convert to DataFrame for storage\n",
    "    stats_records = [\n",
    "        {\"sensor_name\": col, \"mean\": stats[\"mean\"], \"std\": stats[\"std\"], \"computed_at\": datetime.now()}\n",
    "        for col, stats in stats_dict.items()\n",
    "    ]\n",
    "    \n",
    "    stats_df = spark.createDataFrame(stats_records)\n",
    "    \n",
    "    # Write to gold layer\n",
    "    stats_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_STATS_TABLE)\n",
    "    \n",
    "    print(f\"Normalization statistics saved to {GOLD_STATS_TABLE}\")\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "# Compute and store stats\n",
    "norm_stats = compute_and_store_normalization_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce61c961-dfb6-43cc-a519-4c13c3a7968f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_gold_ml_features():\n",
    "    \"\"\"\n",
    "    Create ML-ready features by:\n",
    "    1. Normalizing sensor values using stored statistics\n",
    "    2. Creating sliding windows of size 60 with stride 10\n",
    "    3. Storing as structured format for efficient loading\n",
    "    \n",
    "    Fixed: Properly handles timestamp conversion to avoid Parquet schema issues\n",
    "    \"\"\"\n",
    "    print(\"Creating gold layer ML features...\")\n",
    "    \n",
    "    # Read silver layer and normalization stats\n",
    "    silver_df = spark.table(SILVER_TABLE)\n",
    "    stats_df = spark.table(GOLD_STATS_TABLE)\n",
    "    \n",
    "    # Convert stats to dictionary for easy lookup\n",
    "    stats_dict = {row[\"sensor_name\"]: {\"mean\": row[\"mean\"], \"std\": row[\"std\"]} \n",
    "                  for row in stats_df.collect()}\n",
    "    \n",
    "    # Apply normalization to sensor columns\n",
    "    normalized_df = silver_df\n",
    "    for col, stats in stats_dict.items():\n",
    "        if col in normalized_df.columns:\n",
    "            normalized_df = normalized_df.withColumn(\n",
    "                col,\n",
    "                (F_spark.col(col) - stats[\"mean\"]) / stats[\"std\"]\n",
    "            )\n",
    "    \n",
    "    print(\"Normalization applied to sensor columns\")\n",
    "    \n",
    "    # Convert to pandas for windowing\n",
    "    # This is where we'll be more careful with timestamp handling\n",
    "    pdf = normalized_df.toPandas()\n",
    "    \n",
    "    window_size = CONFIG[\"window_size\"]\n",
    "    stride = CONFIG[\"stride\"]\n",
    "    \n",
    "    label_cols = [\"engine_health\", \"battery_health\", \"urgency\"]\n",
    "    sensor_cols = [c for c in pdf.columns if c not in [\"vehicle_id\", \"timestamp\"] + label_cols]\n",
    "    \n",
    "    print(f\"Creating windows with size={window_size}, stride={stride}\")\n",
    "    print(f\"Sensor columns: {sensor_cols}\")\n",
    "    \n",
    "    window_records = []\n",
    "    window_id = 0\n",
    "    \n",
    "    for vehicle_id, group in pdf.groupby(\"vehicle_id\"):\n",
    "        group = group.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "        n = len(group)\n",
    "        \n",
    "        if n < window_size:\n",
    "            continue  # Skip vehicles with insufficient data\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for start_idx in range(0, n - window_size + 1, stride):\n",
    "            end_idx = start_idx + window_size\n",
    "            window_data = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Extract sensor values as arrays (lists that will become arrays in Spark)\n",
    "            sensor_arrays = {}\n",
    "            for col in sensor_cols:\n",
    "                sensor_arrays[f\"sensor_{col}\"] = window_data[col].tolist()\n",
    "            \n",
    "            # Use last timestep's labels as window labels\n",
    "            last_row = window_data.iloc[-1]\n",
    "            \n",
    "            # CRITICAL FIX: Convert timestamps to strings to avoid Parquet schema issues\n",
    "            # The .isoformat() method converts pandas Timestamps to ISO 8601 format strings\n",
    "            # which are universally compatible with Spark/Parquet\n",
    "            window_start = window_data[\"timestamp\"].iloc[0]\n",
    "            window_end = window_data[\"timestamp\"].iloc[-1]\n",
    "            \n",
    "            record = {\n",
    "                \"window_id\": window_id,\n",
    "                \"vehicle_id\": vehicle_id,\n",
    "                # Convert timestamps to strings - this is the key fix!\n",
    "                \"window_start_time\": window_start.isoformat() if pd.notna(window_start) else None,\n",
    "                \"window_end_time\": window_end.isoformat() if pd.notna(window_end) else None,\n",
    "                **sensor_arrays,  # Unpack all sensor arrays\n",
    "                \"engine_health\": float(last_row[\"engine_health\"]),\n",
    "                \"battery_health\": float(last_row[\"battery_health\"]),\n",
    "                \"urgency\": int(last_row[\"urgency\"]),\n",
    "                \"created_at\": datetime.now().isoformat()  # Also convert this to string\n",
    "            }\n",
    "            \n",
    "            window_records.append(record)\n",
    "            window_id += 1\n",
    "        \n",
    "        if window_id % 1000 == 0 and window_id > 0:\n",
    "            print(f\"  Processed {window_id} windows...\")\n",
    "    \n",
    "    print(f\"Created {len(window_records)} total windows\")\n",
    "    \n",
    "    # Additional safety check: make sure we have windows before trying to write\n",
    "    if len(window_records) == 0:\n",
    "        print(\"ERROR: No windows were created!\")\n",
    "        print(\"This usually means vehicles don't have enough timesteps.\")\n",
    "        print(f\"Window size required: {window_size}\")\n",
    "        print(\"Check your silver table to ensure vehicles have sufficient data.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    # Spark will now properly infer the schema because all timestamps are strings\n",
    "    gold_df = spark.createDataFrame(window_records)\n",
    "    \n",
    "    # Let's verify the schema looks good before writing\n",
    "    print(\"\\nDataFrame schema:\")\n",
    "    gold_df.printSchema()\n",
    "    \n",
    "    # Write to gold layer\n",
    "    gold_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(GOLD_FEATURES_TABLE)\n",
    "    \n",
    "    print(f\"\\n✓ Gold features saved to {GOLD_FEATURES_TABLE}\")\n",
    "    \n",
    "    # Show a sample of what we created\n",
    "    print(\"\\nSample of created features:\")\n",
    "    display(gold_df.select(\"window_id\", \"vehicle_id\", \"window_start_time\", \"engine_health\", \"battery_health\", \"urgency\").limit(5))\n",
    "    \n",
    "    return gold_df\n",
    "\n",
    "# Execute the function\n",
    "gold_features = create_gold_ml_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f08596-5025-4d9a-afb2-7d9dce60907b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a0b53f-25f9-45ee-935e-34dc219d82d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :].to(x.device)\n",
    "\n",
    "class TemporalSensorEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes each sensor's time series into a sensor embedding using a Transformer across time.\n",
    "    Input x shape: (B, T, F)\n",
    "    We'll vectorize by reshaping to (B*F, T, 1) and then project to d_model for Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, T, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(1, d_model)  # we'll apply per sensor value\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=T)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, dim_feedforward=d_model*2)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        B, T, F = x.shape\n",
    "        # reshape to (B*F, T, 1)\n",
    "        x_r = x.permute(0,2,1).contiguous().view(B*F, T, 1)  # (B*F, T, 1)\n",
    "        x_p = self.input_proj(x_r)  # (B*F, T, d_model)\n",
    "        x_pe = self.pos_enc(x_p)    # (B*F, T, d_model)\n",
    "        # transformer expects (T, B', D)\n",
    "        tf_in = x_pe.permute(1,0,2)\n",
    "        tf_out = self.transformer(tf_in)  # (T, B*F, D)\n",
    "        tf_out = tf_out.permute(1,2,0)    # (B*F, D, T)\n",
    "        # pool over time -> (B*F, D, 1)\n",
    "        pooled = self.pool(tf_out).squeeze(-1)  # (B*F, D)\n",
    "        # reshape back to (B, F, D)\n",
    "        sensor_emb = pooled.view(B, F, self.d_model)\n",
    "        return sensor_emb  # (B, F, D)\n",
    "\n",
    "class SensorGraphAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT-style attention across sensors:\n",
    "    Given sensor embeddings H (B, F, D), compute new H' (B, F, D_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.1, leaky_relu_neg_slope=0.2):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.a = nn.Linear(2*out_dim, 1, bias=False)  # attention mechanism\n",
    "        self.leakyrelu = nn.LeakyReLU(leaky_relu_neg_slope)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, H):\n",
    "        # H: (B, n_sensors, D)\n",
    "        B, n_sensors, D = H.shape\n",
    "        Wh = self.W(H)  # (B, n_sensors, D_out)\n",
    "        # compute pairwise attention scores efficiently:\n",
    "        # prepare Wh_i || Wh_j pairs -> shape (B, n_sensors, n_sensors, 2*D_out)\n",
    "        Wh_i = Wh.unsqueeze(2).expand(B, n_sensors, n_sensors, Wh.shape[-1])\n",
    "        Wh_j = Wh.unsqueeze(1).expand(B, n_sensors, n_sensors, Wh.shape[-1])\n",
    "        a_input = torch.cat([Wh_i, Wh_j], dim=-1)  # (B,n_sensors,n_sensors,2*D_out)\n",
    "        e = self.leakyrelu(self.a(a_input).squeeze(-1))  # (B, n_sensors, n_sensors)\n",
    "        # mask self-attention optionally\n",
    "        attention = F.softmax(e, dim=-1)  # softmax over neighbors j\n",
    "        attention = self.dropout(attention)\n",
    "        # aggregate\n",
    "        H_prime = torch.matmul(attention, Wh)  # (B, n_sensors, D_out)\n",
    "        return H_prime\n",
    "\n",
    "class TemporalGraphHealthNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced version that allows configuring the transformer architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, T, F, d_model=64, gat_out=64, hidden=128, \n",
    "                 nhead=4, num_transformer_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Now we pass the transformer config through to the encoder\n",
    "        self.temporal_encoder = TemporalSensorEncoder(\n",
    "            T, \n",
    "            d_model=d_model, \n",
    "            nhead=nhead,  # Pass through the number of attention heads\n",
    "            num_layers=num_transformer_layers,  # Pass through the depth\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.gat = SensorGraphAttention(in_dim=d_model, out_dim=gat_out, dropout=dropout)\n",
    "        \n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(gat_out, hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.engine_head = nn.Linear(hidden, 1)\n",
    "        self.battery_head = nn.Linear(hidden, 1)\n",
    "        self.urgency_head = nn.Linear(hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, F = x.shape\n",
    "        sensor_emb = self.temporal_encoder(x)\n",
    "        gat_out = self.gat(sensor_emb)\n",
    "        pooled = gat_out.mean(dim=1)\n",
    "        z = self.shared_fc(pooled)\n",
    "        engine = torch.sigmoid(self.engine_head(z)).squeeze(-1)\n",
    "        battery = torch.sigmoid(self.battery_head(z)).squeeze(-1)\n",
    "        urgency_logits = self.urgency_head(z)\n",
    "        return {\n",
    "            \"engine\": engine, \n",
    "            \"battery\": battery, \n",
    "            \"urgency_logits\": urgency_logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80571e7c-e4a3-4802-a62e-c6ba11fc399d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utilizing Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845fef15-6f8d-4d63-9759-4a5c9d3a2d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GoldLayerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that reads from the gold layer ML features table.\n",
    "    This is much more efficient than recreating windows on the fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, gold_table_name: str, sensor_cols: List[str]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gold_table_name: Full name of the gold features table\n",
    "            sensor_cols: List of sensor column names (in order)\n",
    "        \"\"\"\n",
    "        print(f\"Loading dataset from {gold_table_name}...\")\n",
    "        \n",
    "        # Read the entire gold table into memory\n",
    "        # For very large datasets, you might want to use Spark's toLocalIterator() instead\n",
    "        gold_df = spark.table(gold_table_name).toPandas()\n",
    "        \n",
    "        self.sensor_cols = sensor_cols\n",
    "        self.samples = []\n",
    "        \n",
    "        # Extract each window as a training sample\n",
    "        for _, row in gold_df.iterrows():\n",
    "            # Reconstruct the feature matrix from sensor arrays\n",
    "            # Shape will be (window_size, num_sensors)\n",
    "            feature_matrix = np.column_stack([\n",
    "                np.array(row[f\"sensor_{col}\"]) for col in sensor_cols\n",
    "            ]).astype(np.float32)\n",
    "            \n",
    "            # Extract labels\n",
    "            labels_continuous = np.array([\n",
    "                row[\"engine_health\"],\n",
    "                row[\"battery_health\"]\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            label_categorical = int(row[\"urgency\"])\n",
    "            \n",
    "            self.samples.append((feature_matrix, labels_continuous, label_categorical))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} training samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    X = torch.stack([torch.tensor(item[0]) for item in batch])\n",
    "    y_continuous = torch.stack([torch.tensor(item[1]) for item in batch])\n",
    "    y_categorical = torch.tensor([item[2] for item in batch]).long()\n",
    "    return X, y_continuous, y_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c94e6ef-200e-42d0-952b-464f9e45fc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring directory structure exists...\n  ✓ Directory ready: /Volumes/project_mayhem/source_data/models\n  ✓ Directory ready: /Volumes/project_mayhem/gold/artifacts\nDirectory structure verified!\n\n"
     ]
    }
   ],
   "source": [
    "# Helper function to ensure directories exist\n",
    "\n",
    "def ensure_directories_exist():\n",
    "    \"\"\"\n",
    "    Ensure all necessary directories and volumes are properly initialized.\n",
    "    This needs to be called before any file write operations.\n",
    "    \"\"\"\n",
    "    print(\"Ensuring directory structure exists...\")\n",
    "    \n",
    "    # These paths need to exist before we can write files to them\n",
    "    directories_to_create = [\n",
    "        MODEL_VOLUME,\n",
    "        ARTIFACTS_VOLUME\n",
    "    ]\n",
    "    \n",
    "    for directory in directories_to_create:\n",
    "        try:\n",
    "            # mkdirs creates the directory if it doesn't exist, does nothing if it does\n",
    "            dbutils.fs.mkdirs(directory)\n",
    "            print(f\"  ✓ Directory ready: {directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error creating directory {directory}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"Directory structure verified!\\n\")\n",
    "\n",
    "    # Ensure that model_filename = \"best_model_checkpoint.pth\" exists such that\n",
    "    # model_path = f\"{MODEL_VOLUME}/{model_filename}\"\n",
    "    # full_path = f\"/dbfs{model_path}\"\n",
    "    \n",
    "\n",
    "# Run this to initialize directories\n",
    "ensure_directories_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721fc954-faf2-4661-b078-55c9c71d649f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba7c813-af14-4122-97d4-9b583a6b2950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training pipeline...\n\nEnsuring directory structure exists...\n  ✓ Directory ready: /Volumes/project_mayhem/source_data/models\n  ✓ Directory ready: /Volumes/project_mayhem/gold/artifacts\nDirectory structure verified!\n\n============================================================\nStarting training run\nRun ID: d1bb83884116439fa44be99c55c03674\n============================================================\nLoading dataset from project_mayhem.gold.ml_features...\nLoaded 27800 training samples\n\nDataset split:\n  Training samples: 23630\n  Validation samples: 4170\n  Batch size: 64\n  Training batches per epoch: 370\n\nModel initialized on cpu\n  Total parameters: 80,261\n  Trainable parameters: 80,261\n\nStarting training for 12 epochs...\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-4d51e760-1804-4a13-bf02-68058364a55e/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 1/12\n  Train Loss: 0.1207 | Val Loss: 0.0140\n  Engine: 0.0137 / 0.0041\n  Battery: 0.0319 / 0.0011\n  Urgency: 0.1501 / 0.0176\nWrote 1456598 bytes.\n  ✓ New best model saved! (val_loss: 0.0140)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:40:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 2/12\n  Train Loss: 0.0137 | Val Loss: 0.0096\n  Engine: 0.0052 / 0.0033\n  Battery: 0.0013 / 0.0010\n  Urgency: 0.0145 / 0.0106\nWrote 1456864 bytes.\n  ✓ New best model saved! (val_loss: 0.0096)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:43:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 3/12\n  Train Loss: 0.0105 | Val Loss: 0.0082\n  Engine: 0.0042 / 0.0029\n  Battery: 0.0012 / 0.0010\n  Urgency: 0.0102 / 0.0086\nWrote 1455926 bytes.\n  ✓ New best model saved! (val_loss: 0.0082)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:46:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 4/12\n  Train Loss: 0.0093 | Val Loss: 0.0076\n  Engine: 0.0036 / 0.0025\n  Battery: 0.0011 / 0.0010\n  Urgency: 0.0092 / 0.0082\nWrote 1456068 bytes.\n  ✓ New best model saved! (val_loss: 0.0076)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:49:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 5/12\n  Train Loss: 0.0087 | Val Loss: 0.0074\n  Engine: 0.0033 / 0.0025\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0086 / 0.0079\nWrote 1456093 bytes.\n  ✓ New best model saved! (val_loss: 0.0074)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:52:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 6/12\n  Train Loss: 0.0084 | Val Loss: 0.0072\n  Engine: 0.0031 / 0.0024\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0084 / 0.0076\nWrote 1456157 bytes.\n  ✓ New best model saved! (val_loss: 0.0072)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:55:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 7/12\n  Train Loss: 0.0083 | Val Loss: 0.0071\n  Engine: 0.0030 / 0.0025\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0084 / 0.0075\nWrote 1455328 bytes.\n  ✓ New best model saved! (val_loss: 0.0071)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 19:57:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 8/12\n  Train Loss: 0.0081 | Val Loss: 0.0070\n  Engine: 0.0029 / 0.0024\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0082 / 0.0075\nWrote 1455965 bytes.\n  ✓ New best model saved! (val_loss: 0.0070)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 20:00:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 9/12\n  Train Loss: 0.0080 | Val Loss: 0.0069\n  Engine: 0.0028 / 0.0023\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0082 / 0.0073\nWrote 1455419 bytes.\n  ✓ New best model saved! (val_loss: 0.0069)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 20:03:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 10/12\n  Train Loss: 0.0079 | Val Loss: 0.0069\n  Engine: 0.0027 / 0.0023\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0082 / 0.0072\nWrote 1455817 bytes.\n  ✓ New best model saved! (val_loss: 0.0069)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 20:06:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 11/12\n  Train Loss: 0.0078 | Val Loss: 0.0068\n  Engine: 0.0027 / 0.0024\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0081 / 0.0071\nWrote 1455148 bytes.\n  ✓ New best model saved! (val_loss: 0.0068)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 20:09:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 12/12\n  Train Loss: 0.0078 | Val Loss: 0.0067\n  Engine: 0.0027 / 0.0023\n  Battery: 0.0011 / 0.0009\n  Urgency: 0.0080 / 0.0069\nWrote 1455119 bytes.\n  ✓ New best model saved! (val_loss: 0.0067)\n    Location: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/15 20:12:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nTRAINING COMPLETED\n============================================================\nBest validation loss: 0.0067 (epoch 12)\nModel saved to: /Volumes/project_mayhem/source_data/models/best_model_checkpoint.pth\nMLflow run ID: d1bb83884116439fa44be99c55c03674\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with MLflow Tracking\n",
    "\n",
    "import io  # for the BytesIO buffer\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Main training function with full MLflow tracking.\n",
    "    Fixed to use Databricks-compatible file saving approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    ensure_directories_exist()\n",
    "    \n",
    "    # Start MLflow run\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "        \n",
    "        # Log all configuration parameters\n",
    "        mlflow.log_params(CONFIG)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"Starting training run\")\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get sensor column names from normalization stats\n",
    "        stats_df = spark.table(GOLD_STATS_TABLE)\n",
    "        sensor_cols = [row[\"sensor_name\"] for row in stats_df.collect()]\n",
    "        mlflow.log_param(\"sensor_columns\", \",\".join(sensor_cols))\n",
    "        mlflow.log_param(\"num_sensors\", len(sensor_cols))\n",
    "        \n",
    "        # Create dataset from gold layer\n",
    "        dataset = GoldLayerDataset(GOLD_FEATURES_TABLE, sensor_cols)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        n_total = len(dataset)\n",
    "        n_val = int(n_total * CONFIG[\"val_split\"])\n",
    "        n_train = n_total - n_val\n",
    "        \n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [n_train, n_val],\n",
    "            generator=torch.Generator().manual_seed(CONFIG[\"random_seed\"])\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=CONFIG[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset split:\")\n",
    "        print(f\"  Training samples: {n_train}\")\n",
    "        print(f\"  Validation samples: {n_val}\")\n",
    "        print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "        print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        device = torch.device(CONFIG[\"device\"])\n",
    "        model = TemporalGraphHealthNet(\n",
    "            T=CONFIG[\"window_size\"],\n",
    "            F=len(sensor_cols),\n",
    "            d_model=64,\n",
    "            gat_out=64,\n",
    "            hidden=128\n",
    "        ).to(device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nModel initialized on {device}\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        mlflow.log_param(\"total_parameters\", total_params)\n",
    "        mlflow.log_param(\"trainable_parameters\", trainable_params)\n",
    "        \n",
    "        # Define loss functions and optimizer\n",
    "        criterion_continuous = nn.MSELoss()\n",
    "        criterion_categorical = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=CONFIG[\"learning_rate\"], \n",
    "            weight_decay=CONFIG[\"weight_decay\"]\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        print(f\"\\nStarting training for {CONFIG['epochs']} epochs...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for epoch in range(CONFIG[\"epochs\"]):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_engine_loss = 0.0\n",
    "            train_battery_loss = 0.0\n",
    "            train_urgency_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (X, y_continuous, y_categorical) in enumerate(train_loader):\n",
    "                X = X.to(device)\n",
    "                y_continuous = y_continuous.to(device)\n",
    "                y_categorical = y_categorical.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(X)\n",
    "                \n",
    "                # Compute individual task losses\n",
    "                loss_engine = criterion_continuous(predictions[\"engine\"], y_continuous[:, 0])\n",
    "                loss_battery = criterion_continuous(predictions[\"battery\"], y_continuous[:, 1])\n",
    "                loss_urgency = criterion_categorical(predictions[\"urgency_logits\"], y_categorical)\n",
    "                \n",
    "                # Combined loss with task weighting\n",
    "                loss = loss_engine + loss_battery + 0.5 * loss_urgency\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                train_loss += loss.item()\n",
    "                train_engine_loss += loss_engine.item()\n",
    "                train_battery_loss += loss_battery.item()\n",
    "                train_urgency_loss += loss_urgency.item()\n",
    "            \n",
    "            # Average training losses\n",
    "            train_loss /= len(train_loader)\n",
    "            train_engine_loss /= len(train_loader)\n",
    "            train_battery_loss /= len(train_loader)\n",
    "            train_urgency_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_engine_loss = 0.0\n",
    "            val_battery_loss = 0.0\n",
    "            val_urgency_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X, y_continuous, y_categorical in val_loader:\n",
    "                    X = X.to(device)\n",
    "                    y_continuous = y_continuous.to(device)\n",
    "                    y_categorical = y_categorical.to(device)\n",
    "                    \n",
    "                    predictions = model(X)\n",
    "                    \n",
    "                    loss_engine = criterion_continuous(predictions[\"engine\"], y_continuous[:, 0])\n",
    "                    loss_battery = criterion_continuous(predictions[\"battery\"], y_continuous[:, 1])\n",
    "                    loss_urgency = criterion_categorical(predictions[\"urgency_logits\"], y_categorical)\n",
    "                    \n",
    "                    loss = loss_engine + loss_battery + 0.5 * loss_urgency\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_engine_loss += loss_engine.item()\n",
    "                    val_battery_loss += loss_battery.item()\n",
    "                    val_urgency_loss += loss_urgency.item()\n",
    "            \n",
    "            # Average validation losses\n",
    "            val_loss /= len(val_loader)\n",
    "            val_engine_loss /= len(val_loader)\n",
    "            val_battery_loss /= len(val_loader)\n",
    "            val_urgency_loss /= len(val_loader)\n",
    "            \n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_engine_loss\": train_engine_loss,\n",
    "                \"train_battery_loss\": train_battery_loss,\n",
    "                \"train_urgency_loss\": train_urgency_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_engine_loss\": val_engine_loss,\n",
    "                \"val_battery_loss\": val_battery_loss,\n",
    "                \"val_urgency_loss\": val_urgency_loss\n",
    "            }, step=epoch)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Engine: {train_engine_loss:.4f} / {val_engine_loss:.4f}\")\n",
    "            print(f\"  Battery: {train_battery_loss:.4f} / {val_battery_loss:.4f}\")\n",
    "            print(f\"  Urgency: {train_urgency_loss:.4f} / {val_urgency_loss:.4f}\")\n",
    "            \n",
    "            # Save best model using Databricks-compatible approach\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                \n",
    "                # Create checkpoint dictionary\n",
    "                checkpoint = {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"config\": CONFIG,\n",
    "                    \"sensor_cols\": sensor_cols,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                # CRITICAL FIX: Save to BytesIO buffer first, then write using dbutils\n",
    "                # This avoids IPython's file handling restrictions\n",
    "                buffer = io.BytesIO()\n",
    "                torch.save(checkpoint, buffer)\n",
    "                buffer.seek(0)  # Reset buffer position to beginning\n",
    "                \n",
    "                # Define the destination path\n",
    "                model_filename = \"best_model_checkpoint.pth\"\n",
    "                model_path = f\"{MODEL_VOLUME}/{model_filename}\"\n",
    "                \n",
    "                # Write the buffer contents using dbutils, which is Databricks-native\n",
    "                # and doesn't go through IPython's modified file operations\n",
    "                dbutils.fs.put(model_path, buffer.read().decode('latin1'), overwrite=True)\n",
    "                \n",
    "                print(f\"  ✓ New best model saved! (val_loss: {val_loss:.4f})\")\n",
    "                print(f\"    Location: {model_path}\")\n",
    "                \n",
    "                # Also log the model to MLflow\n",
    "                # MLflow handles its own file operations safely\n",
    "                mlflow.pytorch.log_model(model, \"model\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f} (epoch {best_epoch+1})\")\n",
    "        print(f\"Model saved to: {model_path}\")\n",
    "        print(f\"MLflow run ID: {run.info.run_id}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return model, best_val_loss, model_path\n",
    "\n",
    "# Run training\n",
    "print(\"Initiating training pipeline...\\n\")\n",
    "trained_model, final_loss, saved_model_path = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6dc20dd-7fa3-426c-bce9-e9e1bcc0f8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Command to Execute It All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad300758-df1e-42da-9384-fc541eab71bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def incremental_training_pipeline(new_data_path: str):\n",
    "    \"\"\"\n",
    "    Complete pipeline for training on new data while preserving old knowledge.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load new CSV into bronze layer (append mode)\n",
    "    2. Reprocess silver layer with combined data\n",
    "    3. Update gold features with new windows\n",
    "    4. Train model with experience replay from previous data\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"INCREMENTAL TRAINING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load new data into bronze\n",
    "    print(\"\\n[1/4] Loading new data into bronze layer...\")\n",
    "    new_df = spark.read.csv(new_data_path, header=True, inferSchema=True)\n",
    "    new_df.write.format(\"delta\").mode(\"append\").saveAsTable(BRONZE_TABLE)\n",
    "    print(f\"  ✓ Added {new_df.count()} new rows to bronze layer\")\n",
    "    \n",
    "    # Step 2: Reprocess silver layer\n",
    "    print(\"\\n[2/4] Reprocessing silver layer...\")\n",
    "    silver_df = create_silver_layer()\n",
    "    print(\"Silver layer updated\")\n",
    "    \n",
    "    # Step 3: Update gold features\n",
    "    print(\"\\n[3/4] Creating new gold features...\")\n",
    "    gold_features = create_gold_ml_features()\n",
    "    print(\"Gold features updated\")\n",
    "    \n",
    "    # Step 4: Train with full dataset\n",
    "    print(\"\\n[4/4] Training model...\")\n",
    "    trained_model, final_loss = train_model()\n",
    "    \n",
    "    print()\n",
    "    print(\"Incremental Training Complete\")\n",
    "    print(f\"Final validation loss: {final_loss:.4f}\")\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "# Note to self: To trigger all this run incremental_training_pipeline(\"/Volumes/project_mayhem/source_data/raw/new_batch.csv\") in next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adb5141-ecf1-49ae-be28-88d39f5c227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5197415783791191,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Training_Process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}